{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"53aa67f675644e179f75fd2f26ef4987":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_f30c94077f3642558dda49496feedf7a","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_406552adc0954c1f90827757c79c6407","IPY_MODEL_be70b1a2e1c640d797191ca49d09caeb"]},"model_module_version":"1.5.0"},"f30c94077f3642558dda49496feedf7a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null},"model_module_version":"1.2.0"},"406552adc0954c1f90827757c79c6407":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_c89753c0161a46c087080f1c329a46f5","_dom_classes":[],"description":"100%","_model_name":"FloatProgressModel","bar_style":"success","max":46827520,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":46827520,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_79ef6df086574666989d2d6b47f36abe"},"model_module_version":"1.5.0"},"be70b1a2e1c640d797191ca49d09caeb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_6d352bb643d24ac58c9319bab69aae10","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 44.7M/44.7M [10:37&lt;00:00, 73.5kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_b0980cfce17d4a998a691ece226351e5"},"model_module_version":"1.5.0"},"c89753c0161a46c087080f1c329a46f5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"},"model_module_version":"1.5.0"},"79ef6df086574666989d2d6b47f36abe":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null},"model_module_version":"1.2.0"},"6d352bb643d24ac58c9319bab69aae10":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"},"model_module_version":"1.5.0"},"b0980cfce17d4a998a691ece226351e5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null},"model_module_version":"1.2.0"},"05b05f1d82a24cae85121a602df4b348":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_cfe4a9685c7945a0bd54e29d77957211","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_83475439d41b409d9e14ec09cf0d9872","IPY_MODEL_0deba5c443a64582b6c6ed008dbf3973"]},"model_module_version":"1.5.0"},"cfe4a9685c7945a0bd54e29d77957211":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null},"model_module_version":"1.2.0"},"83475439d41b409d9e14ec09cf0d9872":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_b1e6014c464f4d41b9592fec106901c3","_dom_classes":[],"description":"100%","_model_name":"FloatProgressModel","bar_style":"success","max":46827520,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":46827520,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_592045747874448ca170e1e11b73ae45"},"model_module_version":"1.5.0"},"0deba5c443a64582b6c6ed008dbf3973":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_77277d7f08f54ea892104177ab09b501","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 44.7M/44.7M [00:00&lt;00:00, 164MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_8dd0dba24d6d4dc0a621ebf0df4ed852"},"model_module_version":"1.5.0"},"b1e6014c464f4d41b9592fec106901c3":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"},"model_module_version":"1.5.0"},"592045747874448ca170e1e11b73ae45":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null},"model_module_version":"1.2.0"},"77277d7f08f54ea892104177ab09b501":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"},"model_module_version":"1.5.0"},"8dd0dba24d6d4dc0a621ebf0df4ed852":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null},"model_module_version":"1.2.0"}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"lJcreEvoLAMR"},"source":["**TO DO:** Make a copy of this notebook in your own Google drive and edit the copy.\n","\n","**TO DO:** Download the data at the following link https://stanfordmlgroup.github.io/competitions/mrnet/ and upload it to your Google Drive.\n","\n","This will download a folder named 'data'.\n","\n","*   The dataset consists of 1,250 knee MRIs with image level labels.\n","*   The training data consists of 1,130 MRIs and the validation data consists of 120 MRIs.\n","*   They are labelled as abnormal, having an acl tear and/or meniscus tear.\n","*   Each MRI exam includes data from the axial, coronal and sagittal plane.\n","*   Axial is a Proton-Density series, coronal is a T1-weighted series and sagittal is T2-weighted series.\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"gAGkxwk-cRff"},"source":["Go to \"Edit\" on the toolbar, then \"Notebook Settings\" and change the hardware accelerator to GPU.\n"]},{"cell_type":"markdown","metadata":{"id":"U1WXxXUT_6_a"},"source":["#Mount Google Drive to access your data\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gOOfjmRD_ppM","outputId":"5a09c35c-e6d3-4eb6-d647-6361a9618f69","executionInfo":{"status":"ok","timestamp":1719996129070,"user_tz":-60,"elapsed":25703,"user":{"displayName":"Fangyijie Wang","userId":"07701241728904216961"}}},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}]},{"cell_type":"markdown","metadata":{"id":"1jByfpneRc97"},"source":["The following code uses a python library named 'torchsample'. This is not installed in Google Colab. We can import it by running the commands in the following cell. The exclamation mark communicates to Google Colab to run the commands in the terminal rather than in Python in the current notebook.\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FIbY0gCaRvMU","outputId":"5f1ad037-6723-414f-9391-51a54f7f206b"},"source":["!pip install -e git+https://github.com/ncullen93/torchsample.git#egg=torchsample\n","!pip install visdom\n","!pip install nibabel\n","!pip install h5py\n","!pip install torchsample\n","!pip install tensorboardX"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Obtaining torchsample from git+https://github.com/ncullen93/torchsample.git#egg=torchsample\n","  Cloning https://github.com/ncullen93/torchsample.git to ./src/torchsample\n","  Running command git clone -q https://github.com/ncullen93/torchsample.git /content/src/torchsample\n","Installing collected packages: torchsample\n","  Running setup.py develop for torchsample\n","Successfully installed torchsample\n","Collecting visdom\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c9/75/e078f5a2e1df7e0d3044749089fc2823e62d029cc027ed8ae5d71fafcbdc/visdom-0.1.8.9.tar.gz (676kB)\n","\u001b[K     |████████████████████████████████| 686kB 36.0MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.8 in /usr/local/lib/python3.7/dist-packages (from visdom) (1.19.5)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from visdom) (1.4.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from visdom) (2.23.0)\n","Requirement already satisfied: tornado in /usr/local/lib/python3.7/dist-packages (from visdom) (5.1.1)\n","Requirement already satisfied: pyzmq in /usr/local/lib/python3.7/dist-packages (from visdom) (22.0.3)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from visdom) (1.15.0)\n","Collecting jsonpatch\n","  Downloading https://files.pythonhosted.org/packages/a3/55/f7c93bae36d869292aedfbcbae8b091386194874f16390d680136edd2b28/jsonpatch-1.32-py2.py3-none-any.whl\n","Collecting torchfile\n","  Downloading https://files.pythonhosted.org/packages/91/af/5b305f86f2d218091af657ddb53f984ecbd9518ca9fe8ef4103a007252c9/torchfile-0.1.0.tar.gz\n","Collecting websocket-client\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/ee/7aa724dc2dbed9b028f463eada5482770c13b7381a0c79457d12b3b62de2/websocket_client-1.0.1-py2.py3-none-any.whl (68kB)\n","\u001b[K     |████████████████████████████████| 71kB 9.6MB/s \n","\u001b[?25hRequirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from visdom) (7.1.2)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->visdom) (2020.12.5)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->visdom) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->visdom) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->visdom) (3.0.4)\n","Collecting jsonpointer>=1.9\n","  Downloading https://files.pythonhosted.org/packages/23/52/05f67532aa922e494c351344e0d9624a01f74f5dd8402fe0d1b563a6e6fc/jsonpointer-2.1-py2.py3-none-any.whl\n","Building wheels for collected packages: visdom, torchfile\n","  Building wheel for visdom (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for visdom: filename=visdom-0.1.8.9-cp37-none-any.whl size=655251 sha256=77a6c3042253eaa24ae4b444ee507b9276e1f045b60e021292a0704d9464d797\n","  Stored in directory: /root/.cache/pip/wheels/70/19/a7/6d589ed967f4dfefd33bc166d081257bd4ed0cb618dccfd62a\n","  Building wheel for torchfile (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for torchfile: filename=torchfile-0.1.0-cp37-none-any.whl size=5712 sha256=629b5e7acd4497ab2f381e1335fb6c43e1c2db376d1c859992479eb066a70103\n","  Stored in directory: /root/.cache/pip/wheels/b1/c3/d6/9a1cc8f3a99a0fc1124cae20153f36af59a6e683daca0a0814\n","Successfully built visdom torchfile\n","Installing collected packages: jsonpointer, jsonpatch, torchfile, websocket-client, visdom\n","Successfully installed jsonpatch-1.32 jsonpointer-2.1 torchfile-0.1.0 visdom-0.1.8.9 websocket-client-1.0.1\n","Requirement already satisfied: nibabel in /usr/local/lib/python3.7/dist-packages (3.0.2)\n","Requirement already satisfied: numpy>=1.12 in /usr/local/lib/python3.7/dist-packages (from nibabel) (1.19.5)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (3.1.0)\n","Requirement already satisfied: numpy>=1.14.5; python_version == \"3.7\" in /usr/local/lib/python3.7/dist-packages (from h5py) (1.19.5)\n","Requirement already satisfied: cached-property; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from h5py) (1.5.2)\n","Requirement already satisfied: torchsample in ./src/torchsample (0.1.3)\n","Collecting tensorboardX\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/07/84/46421bd3e0e89a92682b1a38b40efc22dafb6d8e3d947e4ceefd4a5fabc7/tensorboardX-2.2-py2.py3-none-any.whl (120kB)\n","\u001b[K     |████████████████████████████████| 122kB 23.5MB/s \n","\u001b[?25hRequirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (3.12.4)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (1.19.5)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.8.0->tensorboardX) (57.0.0)\n","Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.8.0->tensorboardX) (1.15.0)\n","Installing collected packages: tensorboardX\n","Successfully installed tensorboardX-2.2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"XoOPghzXShT1"},"source":["#import all libraries\n","import torch.optim as optim\n","import torch\n","import torch.nn as nn\n","from torchvision import models\n","import numpy as np\n","import os\n","import sys\n","import pickle\n","import torch.nn.functional as F\n","import torch.utils.data as data\n","import pandas as pd\n","from torch.autograd import Variable\n","from src.torchsample.torchsample.transforms import RandomRotate, RandomTranslate, RandomFlip, ToTensor, Compose, RandomAffine\n","from torchvision import transforms\n","from tensorboardX import SummaryWriter\n","import math\n","from sklearn import metrics"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DiUxtO2XCqI-"},"source":["#Define your model\n","The model is defined in the class 'Net'. The 'init' function initialises the architecture of the model.\n","\n","The line of code; ```self.pretrained_model = models.resnet18(pretrained=True)``` initialises a pre-trained ResNet18, pre-trained on the ImageNet Dataset. This initialises the weights of the model with the weights for a ResNet18 model that was trained on the ImageNet dataset. This speeds up training.\n","\n","The line of code ```self.classifer = nn.Linear(1000, 1)``` is a fully connected layer that makes the final prediction.\n","\n","After the model is initialised, the forward function is called iteratively throughout the training process. The output size of each line is shown in the code.\n","\n","More information con defining models can be found at https://pytorch.org/vision/stable/models.html"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["53aa67f675644e179f75fd2f26ef4987","f30c94077f3642558dda49496feedf7a","406552adc0954c1f90827757c79c6407","be70b1a2e1c640d797191ca49d09caeb","c89753c0161a46c087080f1c329a46f5","79ef6df086574666989d2d6b47f36abe","6d352bb643d24ac58c9319bab69aae10","b0980cfce17d4a998a691ece226351e5"]},"id":"V7yqBURoddk4","outputId":"b41f3ab4-db6c-45ce-d1a6-37144ce2726b"},"source":["models.resnet18(pretrained=True)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Downloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-5c106cde.pth\n"],"name":"stderr"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"53aa67f675644e179f75fd2f26ef4987","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=46827520.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["ResNet(\n","  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (relu): ReLU(inplace=True)\n","  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n","  (layer1): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (layer2): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (layer3): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (layer4): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n","  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",")"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dghoOuiqe61j","outputId":"7ed4fb54-58c9-4774-ae2b-c3a6a88fe5da"},"source":["list( models.resnet18(pretrained=True).children())[:-1]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False),\n"," BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n"," ReLU(inplace=True),\n"," MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False),\n"," Sequential(\n","   (0): BasicBlock(\n","     (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","     (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","     (relu): ReLU(inplace=True)\n","     (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","     (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","   )\n","   (1): BasicBlock(\n","     (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","     (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","     (relu): ReLU(inplace=True)\n","     (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","     (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","   )\n"," ),\n"," Sequential(\n","   (0): BasicBlock(\n","     (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","     (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","     (relu): ReLU(inplace=True)\n","     (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","     (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","     (downsample): Sequential(\n","       (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","       (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","     )\n","   )\n","   (1): BasicBlock(\n","     (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","     (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","     (relu): ReLU(inplace=True)\n","     (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","     (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","   )\n"," ),\n"," Sequential(\n","   (0): BasicBlock(\n","     (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","     (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","     (relu): ReLU(inplace=True)\n","     (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","     (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","     (downsample): Sequential(\n","       (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","       (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","     )\n","   )\n","   (1): BasicBlock(\n","     (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","     (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","     (relu): ReLU(inplace=True)\n","     (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","     (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","   )\n"," ),\n"," Sequential(\n","   (0): BasicBlock(\n","     (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","     (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","     (relu): ReLU(inplace=True)\n","     (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","     (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","     (downsample): Sequential(\n","       (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","       (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","     )\n","   )\n","   (1): BasicBlock(\n","     (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","     (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","     (relu): ReLU(inplace=True)\n","     (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","     (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","   )\n"," ),\n"," AdaptiveAvgPool2d(output_size=(1, 1))]"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"code","metadata":{"id":"VRAEyCaIApjx"},"source":["#modify the last fully connected layer to output (1) instead of (1000)\n","class Net(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.pretrained_model =  nn.Sequential(*list(models.resnet18(pretrained=True).children())[:-1]  )    # delete the last fc layer.\n","        self.classifer = nn.Linear(512, 1)\n","\n","\n","    def forward(self, x):\n","        # input size of x (1, s, 3, 256, 256) where s is the number of slices in one MRI\n","        x = torch.squeeze(x, dim=0) #output size (s, 3, 256, 256)\n","        x = self.pretrained_model(x) #output size (s, 512)\n","        output = torch.max(x, 0, keepdim=True)[0] #output size (1, 512)\n","        output = self.classifer(output.squeeze(2).squeeze(2)) #output size (1)\n","\n","        return output"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"R-hXicsXnbBI"},"source":["#add another fully connected layer to convert output (1,1000) to (1)\n","class Net(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.pretrained_model =  models.resnet18(pretrained=True)   # delete the last fc layer.\n","        self.classifer = nn.Linear(1000, 1)\n","\n","\n","    def forward(self, x):\n","        # input size of x (1, s, 3, 256, 256) where s is the number of slices in one MRI\n","        x = torch.squeeze(x, dim=0) #output size (s, 3, 256, 256)\n","        x = self.pretrained_model(x) #output size (s, 1000)\n","        output = torch.max(x, 0, keepdim=True)[0] #output size (1, 1000)\n","        output =nn.ReLU()(output)\n","        output = self.classifer(output) #output size (1)\n","\n","        return output"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["05b05f1d82a24cae85121a602df4b348","cfe4a9685c7945a0bd54e29d77957211","83475439d41b409d9e14ec09cf0d9872","0deba5c443a64582b6c6ed008dbf3973","b1e6014c464f4d41b9592fec106901c3","592045747874448ca170e1e11b73ae45","77277d7f08f54ea892104177ab09b501","8dd0dba24d6d4dc0a621ebf0df4ed852"]},"id":"2w3Sd-fon4qr","outputId":"8c059af6-cffe-4e38-ed2b-fe19aec41df0"},"source":["mod=Net()\n","mod"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Downloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-5c106cde.pth\n"],"name":"stderr"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"05b05f1d82a24cae85121a602df4b348","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=46827520.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["Net(\n","  (pretrained_model): Sequential(\n","    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): ReLU(inplace=True)\n","    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n","    (4): Sequential(\n","      (0): BasicBlock(\n","        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (1): BasicBlock(\n","        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (5): Sequential(\n","      (0): BasicBlock(\n","        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (downsample): Sequential(\n","          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (1): BasicBlock(\n","        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (6): Sequential(\n","      (0): BasicBlock(\n","        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (downsample): Sequential(\n","          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (1): BasicBlock(\n","        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (7): Sequential(\n","      (0): BasicBlock(\n","        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (downsample): Sequential(\n","          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (1): BasicBlock(\n","        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (8): AdaptiveAvgPool2d(output_size=(1, 1))\n","  )\n","  (classifer): Linear(in_features=512, out_features=1, bias=True)\n",")"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"QNFlV3zwZHyq"},"source":["TO NOTE:\n","Models defined in Pytorch expect 2D image data in the dimensions (batch size, channels (colours), height of the image, width of the image)"]},{"cell_type":"markdown","metadata":{"id":"wOLCnvQ4HUd2"},"source":["#Create Dataloader\n","The 'init' function initialises the dataloader. This class is responsible for loading the datasets. It takes the 'root_dir', 'task', 'plane', 'train' and 'transform' as input parameters.\n","root_dir - the directory to where the data is stored.\n","\n","task - whether the model is being trained to detect acl tears, meniscus tears or abnormalities. Possible values are 'acl', 'meniscus' or 'abnormal'.\n","\n","plane - whether the model is being trained on axial, coronal or sagittal data. Possible values are 'axial', 'coronal' or 'sagittal'.\n","\n","train - is this the dataloader for the training data or the validation data. Possible values are 'True' to load training data or 'False' to load validation data.\n","\n","transform - a compose function for performing transformations to the images.\n","\n","The init function creates 1) a list of paths to each MRI, 2) a corresponding list of labels that are either ones or zeros and 3) weights.\n","\n","\n","---\n","\n","\n","\n","The __len__ function returns the length of the dataset.\n","\n","\n","---\n","The __getitem__ function is iteratively called throughout the training process. It takes an index as a input parameter. It loads the MRI at the given index from the list of paths defined in the init function. It also returns the label and weight for the MRI at that index.\n","\n"]},{"cell_type":"code","metadata":{"id":"AQzFc45wHOwX"},"source":["class Dataset(data.Dataset):\n","    def __init__(self, root_dir, task, plane, train=False, transform=None):\n","        super().__init__()\n","        self.task = task\n","        self.plane = plane\n","        self.root_dir = root_dir\n","        self.train=train\n","        if self.train == True:\n","            self.folder_path = self.root_dir + 'train/{0}/'.format(plane)\n","            self.records = pd.read_csv(\n","                self.root_dir + 'train-{0}.csv'.format(task), header=None, names=['id', 'label'])\n","        else:\n","            self.folder_path = self.root_dir + 'valid/{0}/'.format(plane)\n","\n","            self.records = pd.read_csv(\n","                self.root_dir + 'valid-{0}.csv'.format(task), header=None, names=['id', 'label'])\n","\n","        self.records['id'] = self.records['id'].map(\n","            lambda i: '0' * (4 - len(str(i))) + str(i))\n","        self.paths = [self.folder_path + filename +\n","                      '.npy' for filename in self.records['id'].tolist()]\n","        self.labels = self.records['label'].tolist()\n","\n","        self.transform = transform\n","\n","        pos = np.sum(self.labels)\n","        neg = len(self.labels) - pos\n","        self.weights = [1, neg / pos]\n","\n","\n","    def __len__(self):\n","        return len(self.paths)\n","\n","    def __getitem__(self, index):\n","        array = np.load(self.paths[index]) #load MRI\n","        label = self.labels[index] #get label of MRI\n","        label = torch.FloatTensor([label]) #convert type from numpy to torch\n","\n","        if self.transform: #if you are transforming it\n","            array = self.transform(array) #transform the image\n","            array = array.numpy()\n","\n","\n","        array = np.stack((array,)*3, axis=1) #the model expects dimensions of (3, 256, 256), the MRIs are greyscale of size (256, 256). Therefore, we stack the image three times to fit the dimensions for the model.\n","        array = torch.FloatTensor(array)\n","\n","        if label.item() == 1:\n","            weight = np.array([self.weights[1]])\n","            weight = torch.FloatTensor(weight)\n","        else:\n","            weight = np.array([self.weights[0]])\n","            weight = torch.FloatTensor(weight)\n","\n","        return array, label, weight"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Wr_G0QuVedGz"},"source":["#Train the model\n","##Define variables\n","**TO DO:** Change directory to where you store your data. Use the toolbar to the side of this page to view your file system."]},{"cell_type":"code","metadata":{"id":"hhB7KNW84hCk"},"source":["directory='./gdrive/MyDrive/MRNet/data/' #directory to the data\n","task = 'acl'\n","plane = 'sagittal'\n","lr = 1e-5 #learning rate\n","num_epochs = 50 # number of epochs"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lZgqHPAEoxv2"},"source":["##Initialise the model, optimiser, scheduler, transformations and data loader."]},{"cell_type":"code","metadata":{"id":"ZOsUXMCT4Y56"},"source":["\n","model = Net() #initialise the model\n","if torch.cuda.is_available(): #if there is a GPU available, put the model on the GPU\n","    model = model.cuda()\n","\n","optimizer = optim.Adam(model.parameters(), lr= lr, weight_decay=0.1) #define the optimiser as Adam\n","\n","scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n","        optimizer, patience=4, factor=.3, threshold=1e-4, verbose=True) #define a scheduler that decreases the learning rate if there has been no reduction in validation loss is four epochs\n","\n","#define a compose function that is a series of transformations on the images.\n","augmentor = Compose([\n","            transforms.Lambda(lambda x: torch.Tensor(x)), #converts from numpy to tensor\n","            RandomRotate(25), #rotate the image by 25 degrees\n","            RandomTranslate([0.11, 0.11]), #blur the edges\n","            RandomFlip(), #flip the image\n","        ])\n","\n","#initialise the train and validation datasets (class we defined earlier) and then initialise a Pytorch's dataloader\n","train_dataset = Dataset(directory, task, plane,\n","                         train=True, transform=augmentor)\n","valid_dataset = Dataset(\n","      directory, task, plane, train=False, transform = None)\n","train_loader = torch.utils.data.DataLoader(\n","    train_dataset, batch_size=1, shuffle=True, num_workers=2, drop_last=False)\n","valid_loader = torch.utils.data.DataLoader(\n","    valid_dataset, batch_size=1, shuffle=-True, num_workers=2, drop_last=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QiJu89o8phOl"},"source":["##Training Loop"]},{"cell_type":"code","metadata":{"id":"V3K5DBz45L6f","colab":{"base_uri":"https://localhost:8080/"},"outputId":"457b3c66-50db-4928-d953-e623cfa248e2"},"source":["early_trigger = 10 #if the validation AUC hasn't increased in ten epochs, stop the training\n","early_stop = 0 #counter for the number of iterations where there has been no increase in validation AUC\n","best_val_auc = 0\n","#for loop for each epoch\n","for epoch in range(num_epochs):\n","      #get learning rate\n","      current_lr = lr\n","\n","      y_preds = []\n","      y_trues = []\n","      losses = []\n","      _ = model.train()\n","      #loop through each MRI in the training set\n","      for i, (image, label, weight) in enumerate(train_loader):\n","          optimizer.zero_grad()\n","\n","          #load all data onto the GPU\n","          if torch.cuda.is_available():\n","              image = image.cuda()\n","              label = label.cuda()\n","              weight = weight.cuda()\n","\n","          label = label[0]\n","          weight = weight[0]\n","\n","          #pass the MRI through the model\n","          prediction = model.forward(image.float()).squeeze(0)\n","\n","          #calculate the loss\n","          loss = torch.nn.BCEWithLogitsLoss(weight=weight)(prediction, label)\n","          loss.backward() #back propagation\n","          optimizer.step()\n","\n","          loss_value = loss.item()\n","          losses.append(loss_value)\n","\n","          probas = torch.sigmoid(prediction) #convert output of model (logits) to a value between zero and one. This can be interpretted as a probability\n","\n","          y_trues.append(int(label[0]))\n","          y_preds.append(probas[0].item())\n","\n","          try:\n","              auc = metrics.roc_auc_score(y_trues, y_preds)\n","          except:\n","              auc = 0.5\n","\n","          train_loss = np.round(np.mean(losses), 4)\n","          train_auc = np.round(auc, 4)\n","\n","      #evaluate the model on the validation data after each epoch\n","      _ = model.eval()\n","      y_trues = []\n","      y_preds = []\n","      losses = []\n","      for i, (image, label, weight) in enumerate(valid_loader):\n","\n","        if torch.cuda.is_available():\n","            image = image.cuda()\n","            label = label.cuda()\n","            weight = weight.cuda()\n","\n","        label = label[0]\n","        weight = weight[0]\n","\n","        prediction = model.forward(image.float()).squeeze(0)\n","\n","        loss = torch.nn.BCEWithLogitsLoss(weight=weight)(prediction, label)\n","\n","        loss_value = loss.item()\n","        losses.append(loss_value)\n","\n","        probas = torch.sigmoid(prediction)\n","\n","        y_trues.append(int(label[0]))\n","        y_preds.append(probas[0].item())\n","\n","        try:\n","            auc = metrics.roc_auc_score(y_trues, y_preds)\n","        except:\n","            auc = 0.5\n","\n","        val_loss = np.round(np.mean(losses), 4)\n","        val_auc = np.round(auc, 4)\n","\n","      if val_auc > best_val_auc:\n","        best_val_auc = val_auc\n","        early_stop=0\n","      else:\n","        early_stop+= 1\n","\n","      if early_stop == early_trigger:\n","        print('Early stopping after {} epochs'.format(epoch))\n","        sys.exit()\n","      scheduler.step(val_loss)\n","\n","      print(\"epoch : {0} | train loss : {1} | train auc {2} | val loss {3} | val auc {4} \".format(\n","          epoch, train_loss, train_auc, val_loss, val_auc))\n","\n","\n","      print('-' * 30)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["epoch : 0 | train loss : 1.1598 | train auc 0.5539 | val loss 0.8801 | val auc 0.6588 \n","------------------------------\n","epoch : 1 | train loss : 1.0492 | train auc 0.6773 | val loss 0.9639 | val auc 0.7135 \n","------------------------------\n","epoch : 2 | train loss : 0.9052 | train auc 0.7975 | val loss 0.77 | val auc 0.7635 \n","------------------------------\n","epoch : 3 | train loss : 0.7987 | train auc 0.8483 | val loss 0.7509 | val auc 0.8058 \n","------------------------------\n","epoch : 4 | train loss : 0.7384 | train auc 0.8734 | val loss 1.3227 | val auc 0.8129 \n","------------------------------\n","epoch : 5 | train loss : 0.6776 | train auc 0.8969 | val loss 0.7783 | val auc 0.7988 \n","------------------------------\n","epoch : 6 | train loss : 0.606 | train auc 0.9202 | val loss 0.9586 | val auc 0.7983 \n","------------------------------\n","epoch : 7 | train loss : 0.5338 | train auc 0.9399 | val loss 0.6189 | val auc 0.8614 \n","------------------------------\n","epoch : 8 | train loss : 0.4995 | train auc 0.9463 | val loss 0.9333 | val auc 0.8634 \n","------------------------------\n","epoch : 9 | train loss : 0.4679 | train auc 0.9518 | val loss 0.679 | val auc 0.8519 \n","------------------------------\n","epoch : 10 | train loss : 0.3802 | train auc 0.9705 | val loss 0.8722 | val auc 0.8771 \n","------------------------------\n","epoch : 11 | train loss : 0.3422 | train auc 0.9755 | val loss 0.749 | val auc 0.8959 \n","------------------------------\n","Epoch    13: reducing learning rate of group 0 to 3.0000e-06.\n","epoch : 12 | train loss : 0.3205 | train auc 0.9785 | val loss 0.9944 | val auc 0.8869 \n","------------------------------\n","epoch : 13 | train loss : 0.2283 | train auc 0.989 | val loss 1.0545 | val auc 0.8967 \n","------------------------------\n","epoch : 14 | train loss : 0.1579 | train auc 0.9964 | val loss 1.2021 | val auc 0.8878 \n","------------------------------\n","epoch : 15 | train loss : 0.1432 | train auc 0.9973 | val loss 1.0732 | val auc 0.9038 \n","------------------------------\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"2X1T8R94fWtq"},"source":[],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"y-vq6WtM38NH"},"source":[],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YlQMa1_mHYp0"},"source":[],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pyBKIuTTJJju"},"source":[],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dbxDZp7mHlvn"},"source":[],"execution_count":null,"outputs":[]}]}