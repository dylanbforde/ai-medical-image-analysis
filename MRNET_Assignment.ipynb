{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54525192",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f2a2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torchsample\n",
    "!pip install visdom\n",
    "!pip install nibabel\n",
    "!pip install h5py\n",
    "!pip install tensorboardX\n",
    "!pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c869f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import pandas as pd\n",
    "from torch.autograd import Variable\n",
    "from torchvision import transforms\n",
    "from tensorboardX import SummaryWriter\n",
    "import math\n",
    "from sklearn import metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd959e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the Net class to include a Dropout layer\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Load the pretrained ResNet18 model\n",
    "        self.pretrained_model = models.resnet18(pretrained=True)\n",
    "        # Modify the last fully connected layer to output (1)\n",
    "        self.classifer = nn.Linear(self.pretrained_model.fc.in_features, 1)\n",
    "        # Add a Dropout layer after the classifier\n",
    "        self.dropout = nn.Dropout(0.5) # Experiment with a dropout rate of 0.5\n",
    "\n",
    "        # Remove the original fully connected layer from the pretrained model\n",
    "        self.pretrained_model.fc = nn.Identity()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input size of x (batch_size, s, 3, 256, 256) where s is the number of slices in one MRI\n",
    "        batch_size, num_slices, channels, height, width = x.size()\n",
    "        # Reshape the input to process slices individually\n",
    "        x = x.view(-1, channels, height, width) # output size (batch_size * num_slices, 3, 256, 256)\n",
    "\n",
    "        x = self.pretrained_model(x) # output size (batch_size * num_slices, 512)\n",
    "\n",
    "        # Reshape back to include the number of slices\n",
    "        x = x.view(batch_size, num_slices, -1) # output size (batch_size, num_slices, 512)\n",
    "\n",
    "        # Apply max pooling across the slices\n",
    "        output = torch.max(x, 1, keepdim=True)[0] # output size (batch_size, 1, 512)\n",
    "\n",
    "        # Remove the extra dimension from keepdim=True and pass through classifier\n",
    "        output = self.classifer(output.squeeze(1)) # output size (batch_size, 1)\n",
    "\n",
    "        # Apply dropout\n",
    "        output = self.dropout(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "# Re-initialize the model with the new architecture\n",
    "model = Net()\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "\n",
    "# Keep the existing optimizer, scheduler, and data loaders\n",
    "# (These are defined in the previous cells and will be used in the training loop)\n",
    "\n",
    "print(\"Model architecture modified to include Dropout layer.\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594ed229",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(data.Dataset):\n",
    "    def __init__(self, root_dir, task, plane, train=False, transform=None):\n",
    "        super().__init__()\n",
    "        self.task = task\n",
    "        self.plane = plane\n",
    "        self.root_dir = root_dir\n",
    "        self.train=train\n",
    "        if self.train == True:\n",
    "            self.folder_path = self.root_dir + 'train/{0}/'.format(plane)\n",
    "            self.records = pd.read_csv(\n",
    "                self.root_dir + 'train-{0}.csv'.format(task), header=None, names=['id', 'label'])\n",
    "        else:\n",
    "            self.folder_path = self.root_dir + 'valid/{0}/'.format(plane)\n",
    "\n",
    "            self.records = pd.read_csv(\n",
    "                self.root_dir + 'valid-{0}.csv'.format(task), header=None, names=['id', 'label'])\n",
    "\n",
    "        self.records['id'] = self.records['id'].map(\n",
    "            lambda i: '0' * (4 - len(str(i))) + str(i))\n",
    "        self.paths = [self.folder_path + filename +\n",
    "                      '.npy' for filename in self.records['id'].tolist()]\n",
    "        self.labels = self.records['label'].tolist()\n",
    "\n",
    "        self.transform = transform\n",
    "\n",
    "        pos = np.sum(self.labels)\n",
    "        neg = len(self.labels) - pos\n",
    "        self.weights = [1, neg / pos]\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        array = np.load(self.paths[index])\n",
    "\n",
    "        label = self.labels[index]\n",
    "        label = torch.FloatTensor([label])\n",
    "\n",
    "        if self.transform:\n",
    "          transformed_slices = []\n",
    "          for i in array:\n",
    "            transformed_slice = self.transform(i)\n",
    "            transformed_slice = transformed_slice.repeat(3, 1, 1)\n",
    "            transformed_slices.append(transformed_slice)\n",
    "          array = torch.stack(transformed_slices)\n",
    "        else:\n",
    "          array = torch.from_numpy(array).float()\n",
    "          array = array.unsqueeze(1)\n",
    "          array = array.repeat(1, 3, 1, 1)\n",
    "\n",
    "        array = array.float()\n",
    "\n",
    "        if label.item() == 1:\n",
    "            weight = np.array([self.weights[1]])\n",
    "            weight = torch.FloatTensor(weight)\n",
    "        else:\n",
    "            weight = np.array([self.weights[0]])\n",
    "            weight = torch.FloatTensor(weight)\n",
    "\n",
    "        if array.shape[0] < 32:\n",
    "            zeros = torch.zeros((32 - array.shape[0], 3, 256, 256))\n",
    "            array = torch.cat((array, zeros), 0)\n",
    "\n",
    "        elif array.shape[0] > 32:\n",
    "            array = array[:32, :, :, :]\n",
    "\n",
    "        return array, label, weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5bcb6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # Define the hyperparameters to tune\n",
    "    lr = trial.suggest_float('lr', 1e-6, 1e-4, log=True)\n",
    "    weight_decay = trial.suggest_float('weight_decay', 1e-5, 1e-3, log=True)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [8, 16, 32]) # Example batch sizes\n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.6) # Tune dropout rate\n",
    "    trial_number = trial.number\n",
    "\n",
    "    # --- Model Initialization ---\n",
    "    model = Net()\n",
    "    # Update the dropout rate in the model with the suggested value from Optuna\n",
    "    model.dropout.p = dropout_rate\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.cuda()\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, patience=2, factor=.3, threshold=1e-4, verbose=False) # Reduced patience for tuning\n",
    "\n",
    "    # --- Data Loading ---\n",
    "    directory = \"/content/drive/Shared drives/MRNet Group Assignment/MRI Data/\"\n",
    "    task = 'acl'\n",
    "    plane = 'sagittal'\n",
    "\n",
    "    augmentor = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.RandomRotation(25),\n",
    "        transforms.RandomAffine(degrees=0, translate=(0.11, 0.11)),\n",
    "        transforms.RandomHorizontalFlip()\n",
    "    ])\n",
    "\n",
    "    train_dataset = Dataset(directory, task, plane, train=True, transform=augmentor)\n",
    "    valid_dataset = Dataset(directory, task, plane, train=False, transform = None)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=batch_size, shuffle=True, num_workers=2, drop_last=False)\n",
    "    valid_loader = torch.utils.data.DataLoader(\n",
    "        valid_dataset, batch_size=batch_size, shuffle=False, num_workers=2, drop_last=False)\n",
    "\n",
    "\n",
    "    # --- Training Loop (Shortened for Tuning) ---\n",
    "    num_tuning_epochs = 5 # Reduced number of epochs for faster tuning trials\n",
    "    best_val_auc = 0\n",
    "\n",
    "    for epoch in range(num_tuning_epochs):\n",
    "        model.train()\n",
    "        y_preds = []\n",
    "        y_trues = []\n",
    "        losses = []\n",
    "        for i, (image, label, weight) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            if torch.cuda.is_available():\n",
    "                image = image.cuda()\n",
    "                label = label.cuda()\n",
    "                weight = weight.cuda()\n",
    "\n",
    "            prediction = model.forward(image.float())\n",
    "            loss = torch.nn.BCEWithLogitsLoss(weight=weight)(prediction, label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            probas = torch.sigmoid(prediction)\n",
    "            y_trues.extend(label.cpu().tolist())\n",
    "            y_preds.extend(probas.cpu().tolist())\n",
    "\n",
    "        # Evaluate on validation set\n",
    "        model.eval()\n",
    "        y_trues_val = []\n",
    "        y_preds_val = []\n",
    "        with torch.no_grad():\n",
    "            for i, (image, label, weight) in enumerate(valid_loader):\n",
    "                if torch.cuda.is_available():\n",
    "                    image = image.cuda()\n",
    "                    label = label.cuda()\n",
    "                    weight = weight.cuda()\n",
    "\n",
    "                prediction = model.forward(image.float())\n",
    "                probas = torch.sigmoid(prediction)\n",
    "                y_trues_val.extend(label.cpu().tolist())\n",
    "                y_preds_val.extend(probas.cpu().tolist())\n",
    "\n",
    "        try:\n",
    "            val_auc = metrics.roc_auc_score(y_trues_val, y_preds_val)\n",
    "        except:\n",
    "            val_auc = 0.5 # Handle case with only one class\n",
    "\n",
    "        # Report intermediate objective value to Optuna\n",
    "        trial.report(val_auc, epoch)\n",
    "\n",
    "        # Handle pruning based on the intermediate value\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "        if val_auc > best_val_auc:\n",
    "            best_val_auc = val_auc\n",
    "\n",
    "    model_save_path = f'/content/drive/MyDrive/my_acl_sagittal_model_trial_{trial_number}.pth'\n",
    "\n",
    "    # Save the model's state dictionary\n",
    "    torch.save(model.state_dict(), model_save_path)\n",
    "\n",
    "    return best_val_auc # Return the best validation AUC for this trial\n",
    "\n",
    "# --- Run the Optuna study ---\n",
    "# Specify the number of trials to run. Keep this small initially to manage computation cost.\n",
    "n_trials = 10 # You can increase this number for a more extensive search, however my compute credits are low\n",
    "\n",
    "print(f\"Running Optuna study for {n_trials} trials...\")\n",
    "\n",
    "# Create a study object and specify the direction (maximize validation AUC)\n",
    "study = optuna.create_study(direction='maximize')\n",
    "\n",
    "# Run the optimization study\n",
    "study.optimize(objective, n_trials=n_trials)\n",
    "\n",
    "print(\"\\nOptuna study finished.\")\n",
    "\n",
    "# Print the best trial's hyperparameters and value\n",
    "print(\"Best trial:\")\n",
    "print(f\"  Value: {study.best_trial.value}\")\n",
    "print(\"  Params: \")\n",
    "for key, value in study.best_trial.params.items():\n",
    "    print(f\"    {key}: {value}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
